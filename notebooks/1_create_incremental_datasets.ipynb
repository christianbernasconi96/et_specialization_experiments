{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create partitions for pretraining datasets and incremental datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../../entity_typing_analysis/')\n",
    "import utils\n",
    "\n",
    "# imports\n",
    "import os\n",
    "import numpy as np\n",
    "import json \n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "import shutil\n",
    "# set main directories\n",
    "DATA = 'few_NERD'\n",
    "TRAIN_DATA = 'train.json'\n",
    "DEV_DATA = 'dev.json'\n",
    "# TEST_DATA = f\"test{'-12k' if DATA == 'bbn' else ''}.json\"\n",
    "TEST_DATA = f\"test.json\"\n",
    "# SRC_DATA_DIR = f\"/home/mvimercati/entity_typing/datasets/ren_et_al/{DATA}{'/relabel' if DATA == 'figer' else ''}\"\n",
    "SRC_DATA_DIR = f\"/home/remote_hdd/datasets_for_incremental_training/{DATA}\"\n",
    "DST_DATA_DIR = f'/home/remote_hdd/datasets_for_incremental_training/{DATA}'\n",
    "# DST_DATA_DIR = os.path.expanduser(f'./{DATA}/complete')\n",
    "ONTOLOGY_PATH = os.path.join(SRC_DATA_DIR, f\"all_types.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load ontology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thing\n",
      "├── /art\n",
      "│   ├── /art/broadcastprogram\n",
      "│   ├── /art/film\n",
      "│   ├── /art/music\n",
      "│   ├── /art/other\n",
      "│   ├── /art/painting\n",
      "│   └── /art/writtenart\n",
      "├── /building\n",
      "│   ├── /building/airport\n",
      "│   ├── /building/hospital\n",
      "│   ├── /building/hotel\n",
      "│   ├── /building/library\n",
      "│   ├── /building/other\n",
      "│   ├── /building/restaurant\n",
      "│   ├── /building/sportsfacility\n",
      "│   └── /building/theater\n",
      "├── /event\n",
      "│   ├── /event/attack_battle_war_militaryconflict\n",
      "│   ├── /event/disaster\n",
      "│   ├── /event/election\n",
      "│   ├── /event/other\n",
      "│   ├── /event/protest\n",
      "│   └── /event/sportsevent\n",
      "├── /location\n",
      "│   ├── /location/GPE\n",
      "│   ├── /location/bodiesofwater\n",
      "│   ├── /location/island\n",
      "│   ├── /location/mountain\n",
      "│   ├── /location/other\n",
      "│   ├── /location/park\n",
      "│   └── /location/road_railway_highway_transit\n",
      "├── /organization\n",
      "│   ├── /organization/company\n",
      "│   ├── /organization/education\n",
      "│   ├── /organization/government_governmentagency\n",
      "│   ├── /organization/media_newspaper\n",
      "│   ├── /organization/other\n",
      "│   ├── /organization/politicalparty\n",
      "│   ├── /organization/religion\n",
      "│   ├── /organization/showorganization\n",
      "│   ├── /organization/sportsleague\n",
      "│   └── /organization/sportsteam\n",
      "├── /other\n",
      "│   ├── /other/astronomything\n",
      "│   ├── /other/award\n",
      "│   ├── /other/biologything\n",
      "│   ├── /other/chemicalthing\n",
      "│   ├── /other/currency\n",
      "│   ├── /other/disease\n",
      "│   ├── /other/educationaldegree\n",
      "│   ├── /other/god\n",
      "│   ├── /other/language\n",
      "│   ├── /other/law\n",
      "│   ├── /other/livingthing\n",
      "│   └── /other/medical\n",
      "├── /person\n",
      "│   ├── /person/actor\n",
      "│   ├── /person/artist_author\n",
      "│   ├── /person/athlete\n",
      "│   ├── /person/director\n",
      "│   ├── /person/other\n",
      "│   ├── /person/politician\n",
      "│   ├── /person/scholar\n",
      "│   └── /person/soldier\n",
      "└── /product\n",
      "    ├── /product/airplane\n",
      "    ├── /product/car\n",
      "    ├── /product/food\n",
      "    ├── /product/game\n",
      "    ├── /product/other\n",
      "    ├── /product/ship\n",
      "    ├── /product/software\n",
      "    ├── /product/train\n",
      "    └── /product/weapon\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load ontology\n",
    "type2id = utils.load_ontology(os.path.join(SRC_DATA_DIR, 'all_types.txt'))\n",
    "types = list(type2id.keys())\n",
    "# create hierarchy tree\n",
    "tree = utils.create_tree(ONTOLOGY_PATH)\n",
    "tree.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relabel(path_to_read, dirpath_to_write, tree, labels_to_remove = [], recreate_dirs=True, only_pretraining=False):\n",
    "  # prepare empty main folder\n",
    "  if recreate_dirs:\n",
    "    if os.path.exists(dirpath_to_write):\n",
    "      shutil.rmtree(dirpath_to_write)\n",
    "    else:\n",
    "      os.makedirs(dirpath_to_write, exist_ok=True)\n",
    "\n",
    "  # prepare labels to remove\n",
    "  labels_to_remove = set(labels_to_remove)\n",
    "  \n",
    "  # prepare pretraining filepath\n",
    "  postfix = path_to_read.split('/')[-1].replace('.json','')\n",
    "  pretraining_path = os.path.join(dirpath_to_write, f'pretraining_{postfix}.json')\n",
    "\n",
    "  # prepare empty folder for each father of incremental types\n",
    "  if not only_pretraining:\n",
    "    incremental_paths = {}\n",
    "    for label in labels_to_remove:\n",
    "      # prepare dir name\n",
    "      father = tree.parent(label).identifier[1:].replace('/', '_')\n",
    "      incremental_dirpath = os.path.join(dirpath_to_write, f'sons_of_{father}')\n",
    "      \n",
    "      # create dir\n",
    "      if not os.path.exists(incremental_dirpath):\n",
    "        os.makedirs(incremental_dirpath)\n",
    "\n",
    "      # map each type to its incremental filepath\n",
    "      incremental_filepath = os.path.join(incremental_dirpath, f\"incremental_{postfix}_{label.split('/')[-1]}.json\")\n",
    "      incremental_paths[label] = incremental_filepath\n",
    "\n",
    "\n",
    "  with open(path_to_read, 'r') as src, open(pretraining_path, 'a') as pretraining_dst:\n",
    "    for t in tqdm(src.readlines()):\n",
    "      \n",
    "      # read example\n",
    "      example = json.loads(t)\n",
    "      \n",
    "      # create pretraining example\n",
    "      labels = set(example['y_str'])\n",
    "      labels_to_keep = labels - labels_to_remove\n",
    "      labels_removed = labels.intersection(labels_to_remove)\n",
    "      example['y_str'] = list(labels_to_keep)\n",
    "      pretraining_dst.write(json.dumps(example)+'\\n')\n",
    "\n",
    "      if not only_pretraining:\n",
    "        # create incremental examples\n",
    "        for label_incremental in labels_removed:\n",
    "          with open(incremental_paths[label_incremental], 'a') as incremental_dst:\n",
    "            example_incremental = deepcopy(example)\n",
    "            example_incremental['y_str'].append(label_incremental)\n",
    "            incremental_dst.write(json.dumps(example_incremental)+'\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete family"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dst_data_dir = os.path.join(DST_DATA_DIR, 'complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66 leaves: ['/art/broadcastprogram', '/art/film', '/art/music', '/art/other', '/art/painting', '/art/writtenart', '/building/airport', '/building/hospital', '/building/hotel', '/building/library', '/building/other', '/building/restaurant', '/building/sportsfacility', '/building/theater', '/event/attack_battle_war_militaryconflict', '/event/disaster', '/event/election', '/event/other', '/event/protest', '/event/sportsevent', '/location/GPE', '/location/bodiesofwater', '/location/island', '/location/mountain', '/location/other', '/location/park', '/location/road_railway_highway_transit', '/organization/company', '/organization/education', '/organization/government_governmentagency', '/organization/media_newspaper', '/organization/other', '/organization/politicalparty', '/organization/religion', '/organization/showorganization', '/organization/sportsleague', '/organization/sportsteam', '/other/astronomything', '/other/award', '/other/biologything', '/other/chemicalthing', '/other/currency', '/other/disease', '/other/educationaldegree', '/other/god', '/other/language', '/other/law', '/other/livingthing', '/other/medical', '/person/actor', '/person/artist_author', '/person/athlete', '/person/director', '/person/other', '/person/politician', '/person/scholar', '/person/soldier', '/product/airplane', '/product/car', '/product/food', '/product/game', '/product/other', '/product/ship', '/product/software', '/product/train', '/product/weapon']\n"
     ]
    }
   ],
   "source": [
    "leaves_nth = tree.filter_nodes(lambda x : len(tree.children(x.identifier)) == 0\n",
    "                                      and tree.depth(x.identifier) == tree.depth())\n",
    "leaves_nth = [*map(lambda x : x.identifier, leaves_nth)]\n",
    "print(len(leaves_nth), 'leaves:', leaves_nth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/remote_hdd/datasets_for_incremental_training/few_NERD/train.json'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join(SRC_DATA_DIR, TRAIN_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 338407/338407 [16:14<00:00, 347.17it/s] \n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "relabel(path_to_read=os.path.join(SRC_DATA_DIR, TRAIN_DATA),\n",
    "        dirpath_to_write=dst_data_dir,\n",
    "        tree=tree,\n",
    "        labels_to_remove=leaves_nth,\n",
    "        recreate_dirs=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1980/1980 [00:00<00:00, 25449.39it/s]\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "relabel(path_to_read=os.path.join(SRC_DATA_DIR, TEST_DATA),\n",
    "        dirpath_to_write=dst_data_dir,\n",
    "        tree=tree,\n",
    "        labels_to_remove=leaves_nth,\n",
    "        recreate_dirs=False,\n",
    "        only_pretraining=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single child"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dst_data_dir = os.path.join(DST_DATA_DIR, 'single_child')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_to_remove = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200662/200662 [03:21<00:00, 994.33it/s] \n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "relabel(path_to_read=os.path.join(SRC_DATA_DIR, TRAIN_DATA),\n",
    "        dirpath_to_write=dst_data_dir,\n",
    "        tree=tree,\n",
    "        labels_to_remove=labels_to_remove,\n",
    "        recreate_dirs=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2027/2027 [00:00<00:00, 39168.58it/s]\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "relabel(path_to_read=os.path.join(SRC_DATA_DIR, TEST_DATA),\n",
    "        dirpath_to_write=dst_data_dir,\n",
    "        tree=tree,\n",
    "        labels_to_remove=labels_to_remove,\n",
    "        recreate_dirs=False,\n",
    "        only_pretraining=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('allennlp_venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d1e9dcee44032020c09ec3581d25e14d5c935065f63f4e5d5ebaabcd7ccba5a4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
